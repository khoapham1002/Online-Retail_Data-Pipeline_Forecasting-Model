{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db464ca5",
   "metadata": {},
   "source": [
    "## Background Information\n",
    "\n",
    "Prepare `\"orders_data.parquet\"` so that it can be used to build a forecasting model.\n",
    "\n",
    "* Clean the orders dataset as per the requirements specified in the Workbook.\n",
    "* Save the updated file as `orders_data_clean.parquet`.\n",
    "\n",
    "<br>\n",
    "\n",
    "As a Data Engineer at an electronics e-commerce company, Voltmart, you have been requested by a peer Machine Learning team to clean the data containing the information about orders made last year. They are planning to further use this cleaned data to build a demand forecasting model. To achieve this, they have shared their requirements regarding the desired output table format.\n",
    "\n",
    "An analyst shared a parquet file called `\"orders_data.parquet\"` for you to clean and preprocess. \n",
    "\n",
    "You can see the dataset schema below along with the **cleaning requirements**:\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "`orders_data.parquet`\n",
    "\n",
    "| column | data type | description | cleaning requirements | \n",
    "|--------|-----------|-------------|-----------------------|\n",
    "| `order_date` | `timestamp` | Date and time when the order was made | _Modify: Remove orders placed between 12am and 5am (inclusive); convert from timestamp to date_ |\n",
    "| `time_of_day` | `string` | Period of the day when the order was made | _New column containing (lower bound inclusive, upper bound exclusive): \"morning\" for orders placed 5-12am, \"afternoon\" for orders placed 12-6pm, and \"evening\" for 6-12pm_ |\n",
    "| `order_id` | `long` | Order ID | _N/A_ |\n",
    "| `product` | `string` | Name of a product ordered | _Remove rows containing \"TV\" as the company has stopped selling this product; ensure all values are lowercase_ |\n",
    "| `product_ean` | `double` | Product ID | _N/A_ |\n",
    "| `category` | `string` | Broader category of a product | _Ensure all values are lowercase_ |\n",
    "| `purchase_address` | `string` | Address line where the order was made (\"House Street, City, State Zipcode\") | _N/A_ |\n",
    "| `purchase_state` | `string` | US State of the purchase address | _New column containing: the State that the purchase was ordered from_ |\n",
    "| `quantity_ordered` | `long` | Number of product units ordered | _N/A_ |\n",
    "| `price_each` | `double` | Price of a product unit | _N/A_ |\n",
    "| `cost_price` | `double` | Cost of production per product unit | _N/A_ |\n",
    "| `turnover` | `double` | Total amount paid for a product (quantity x price) | _N/A_ |\n",
    "| `margin` | `double` | Profit made by selling a product (turnover - cost) | _N/A_ |\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b55d87-f8fd-4e8a-9ef6-6fe91f4de5f3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 6367,
    "lastExecutedAt": 1736459562075,
    "lastExecutedByKernel": "021d7f98-ac6b-4d4d-a4a6-963852209939",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from pyspark.sql import (\n    SparkSession,\n    types,\n    functions as F,\n)\n\nspark = (\n    SparkSession\n    .builder\n    .appName('cleaning_orders_dataset_with_pyspark')\n    .getOrCreate()\n)",
    "outputsMetadata": {
     "0": {
      "height": 101,
      "type": "stream"
     },
     "1": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/10 22:32:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/10 22:32:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import (\n",
    "    SparkSession,\n",
    "    types,\n",
    "    functions as F,\n",
    ")\n",
    "\n",
    "# Initiate a Spark session\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('cleaning_orders_dataset_with_pyspark')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa903d28-82b2-4c39-90b1-3d9a9421fb6d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 54,
      "type": "stream"
     },
     "1": {
      "height": 50,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "7cd495a1-410c-4c4e-851f-04dcd3193f65",
        "nodeType": "const"
       },
       "quickFilterText": ""
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# IMPORT DATA\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Read data from the parquet file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m orders_data \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data_raw/orders_data.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43morders_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-kdpham1002@gmail.com/My Drive/Coding Workspace/My Projects_Repos/Ecommerce-Retail_Data-Pipeline_Forecasting-Model/.venv/lib/python3.13/site-packages/pyspark/sql/pandas/conversion.py:86\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _create_converter_to_pandas\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[0;32m---> 86\u001b[0m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     90\u001b[0m jconf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_jconf\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-kdpham1002@gmail.com/My Drive/Coding Workspace/My Projects_Repos/Ecommerce-Retail_Data-Pipeline_Forecasting-Model/.venv/lib/python3.13/site-packages/pyspark/sql/pandas/utils.py:24\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# TODO(HyukjinKwon): Relocate and deduplicate the version specification.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m minimum_pandas_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "# IMPORT DATA\n",
    "# Read data from the parquet file\n",
    "\n",
    "orders_data = spark.read.parquet('../data_raw/orders_data.parquet')\n",
    "orders_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e318a0c9-ecbb-4b0b-8292-99ae957f524a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 5163,
    "lastExecutedAt": 1736459574272,
    "lastExecutedByKernel": "021d7f98-ac6b-4d4d-a4a6-963852209939",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Start here, using as many cells as you require\nfrom pyspark.sql import (\n    SparkSession,\n    types,\n    functions as F,\n)\n\n# Initiate a Spark session\nspark = (\n    SparkSession\n    .builder\n    .appName('cleaning_orders_dataset_with_pyspark')\n    .getOrCreate()\n)\n\n# IMPORT DATA\n\n# Read data from the parquet file\norders_data = spark.read.parquet('orders_data.parquet')\n\n# DATA CLEANING AND PREPROCESSING\n\norders_data = (\n    orders_data\n    # Create a new column time_of_day\n    .withColumn(\n        'time_of_day',\n        # When/otherwise (similar to case/when/else) statements extracting hour from timestamp\n        F.when((F.hour('order_date') >= 0) & (F.hour('order_date') <= 5), 'night')\n         .when((F.hour('order_date') >= 6) & (F.hour('order_date') <= 11), 'morning')\n         .when((F.hour('order_date') >= 12) & (F.hour('order_date') <= 17), 'afternoon')\n         .when((F.hour('order_date') >= 18) & (F.hour('order_date') <= 23), 'evening')\n        # You can keep the otherwise statement as None to validate whether the conditions are exhaustive\n         .otherwise(None)\n    )\n    # Filter by time of day\n    .filter(\n        F.col('time_of_day') != 'night'\n    )\n    # Cast order_date to date as it is originally a timestamp\n    .withColumn(\n        'order_date',\n        F.col('order_date').cast(types.DateType())\n    )\n)\n\n\norders_data = (\n    orders_data\n    # Make product and category columns lowercase\n    .withColumn(\n        'product',\n        F.lower('product')\n    )\n    .withColumn(\n        'category',\n        F.lower('category')\n    )\n    # Remove rows where product column contains \"tv\" (as you have already made it lowercase)\n    .filter(\n        ~F.col('product').contains('tv')\n    )\n)\n\n\norders_data = (\n    orders_data\n    # First you split the purchase address by space (\" \")\n    .withColumn(\n        'address_split',\n        F.split('purchase_address', ' ')\n    )\n    # If you look at the address lines, you can see that the state abbreviation is always at the 2nd last position\n    .withColumn(\n        'purchase_state',\n        F.col('address_split').getItem(F.size('address_split') - 2)\n    )\n    # Dropping address_split columns as it is a temporary technical column\n    .drop('address_split')\n)\n\n# Use distinct and count to calculate the number of unique values\nn_states = (\n    orders_data\n    .select('purchase_state')\n    .distinct()\n    .count()\n)\n\n\n# EXPORT\n\n# Export the resulting table to parquet format with the new name\n(\n    orders_data\n    .write\n    .parquet(\n        'orders_data_clean.parquet',\n        mode='overwrite',\n    )\n)\n",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# DATA CLEANING AND PREPROCESSING\n",
    "\n",
    "orders_data = (\n",
    "    orders_data\n",
    "    # Create a new column time_of_day\n",
    "    .withColumn(\n",
    "        'time_of_day',\n",
    "        # When/otherwise (similar to case/when/else) statements extracting hour from timestamp\n",
    "        F.when((F.hour('order_date') >= 0) & (F.hour('order_date') <= 5), 'night')\n",
    "         .when((F.hour('order_date') >= 6) & (F.hour('order_date') <= 11), 'morning')\n",
    "         .when((F.hour('order_date') >= 12) & (F.hour('order_date') <= 17), 'afternoon')\n",
    "         .when((F.hour('order_date') >= 18) & (F.hour('order_date') <= 23), 'evening')\n",
    "        # You can keep the otherwise statement as None to validate whether the conditions are exhaustive\n",
    "         .otherwise(None)\n",
    "    )\n",
    "    # Filter by time of day\n",
    "    .filter(\n",
    "        F.col('time_of_day') != 'night'\n",
    "    )\n",
    "    # Cast order_date to date as it is originally a timestamp\n",
    "    .withColumn(\n",
    "        'order_date',\n",
    "        F.col('order_date').cast(types.DateType())\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "orders_data = (\n",
    "    orders_data\n",
    "    # Make product and category columns lowercase\n",
    "    .withColumn(\n",
    "        'product',\n",
    "        F.lower('product')\n",
    "    )\n",
    "    .withColumn(\n",
    "        'category',\n",
    "        F.lower('category')\n",
    "    )\n",
    "    # Remove rows where product column contains \"tv\" (as you have already made it lowercase)\n",
    "    .filter(\n",
    "        ~F.col('product').contains('tv')\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "orders_data = (\n",
    "    orders_data\n",
    "    # First you split the purchase address by space (\" \")\n",
    "    .withColumn(\n",
    "        'address_split',\n",
    "        F.split('purchase_address', ' ')\n",
    "    )\n",
    "    # If you look at the address lines, you can see that the state abbreviation is always at the 2nd last position\n",
    "    .withColumn(\n",
    "        'purchase_state',\n",
    "        # F.col('address_split').getItem(F.size('address_split') - 2)\n",
    "        F.col('address_split')[F.size('address_split') - 2]\n",
    "    )\n",
    "    # Dropping address_split columns as it is a temporary technical column\n",
    "    .drop('address_split')\n",
    ")\n",
    "\n",
    "# Use distinct and count to calculate the number of unique values\n",
    "n_states = (\n",
    "    orders_data\n",
    "    .select('purchase_state')\n",
    "    .distinct()\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914e7342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# EXPORT\n",
    "# Export the resulting table to parquet format with the new name\n",
    "(\n",
    "    orders_data\n",
    "    .write\n",
    "    .parquet(\n",
    "        '../data_cleaned/orders_data.parquet_cleaned',\n",
    "        mode='overwrite',\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter nbconvert --to HTML orders-dataset-cleaning.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
